[('num_adapters', 0), ('alpha', 1), ('req_rate', 2), ('cv', 1), ('duration', 15), ('input_range', [1, 5]), ('output_range', [1, 5])]
get_adapter_dirs input and output 0 ['dummy-lora-13b-rank-64', 'dummy-lora-13b-rank-32', 'dummy-lora-13b-rank-16'] None ['dummy-lora-13b-rank-64-0', 'dummy-lora-13b-rank-32-0', 'dummy-lora-13b-rank-16-0']
========
generating requests v2 1 1 2 1 15 [1, 5] [1, 5] [('huggyllama/llama-13b', None)] 42
[16, 1024, 2040, 16, 16, 16, 16, 16] [2040, 8, 8, 8, 8, 8, 8, 8]
<bound method Request.__repr__ of req_id=0, model_dir=huggyllama/llama-13b, adapter_dir=None, prompt_len=16, output_len=2040, req_time=0.45954107681363227>
<bound method Request.__repr__ of req_id=1, model_dir=huggyllama/llama-13b, adapter_dir=None, prompt_len=1024, output_len=8, req_time=1.0751661076659274>
<bound method Request.__repr__ of req_id=2, model_dir=huggyllama/llama-13b, adapter_dir=None, prompt_len=2040, output_len=8, req_time=1.0855657616654966>
<bound method Request.__repr__ of req_id=3, model_dir=huggyllama/llama-13b, adapter_dir=None, prompt_len=16, output_len=8, req_time=2.8373444992446526>
<bound method Request.__repr__ of req_id=4, model_dir=huggyllama/llama-13b, adapter_dir=None, prompt_len=16, output_len=8, req_time=3.73055927092199>
<bound method Request.__repr__ of req_id=5, model_dir=huggyllama/llama-13b, adapter_dir=None, prompt_len=16, output_len=8, req_time=3.849903083546464>
<bound method Request.__repr__ of req_id=6, model_dir=huggyllama/llama-13b, adapter_dir=None, prompt_len=16, output_len=8, req_time=3.9502425779212955>
<bound method Request.__repr__ of req_id=7, model_dir=huggyllama/llama-13b, adapter_dir=None, prompt_len=16, output_len=8, req_time=4.051548289337424>
avg_len: 657.0 avg_prompt_len: 395.0 avg_output_len: 262.0
ready to request generate API http://localhost:8000/generate_stream model_dir huggyllama/llama-13b adapter_dir None len_prompt 96
ready to request generate API http://localhost:8000/generate_stream model_dir huggyllama/llama-13b adapter_dir None len_prompt 6144
ready to request generate API http://localhost:8000/generate_stream model_dir huggyllama/llama-13b adapter_dir None len_prompt 12240
req_id 2 prompt_len 2040 output_len 8 request_latency 0.61 s, first_token_latency 0.41 s
req_id 1 prompt_len 1024 output_len 8 request_latency 0.62 s, first_token_latency 0.42 s
ready to request generate API http://localhost:8000/generate_stream model_dir huggyllama/llama-13b adapter_dir None len_prompt 96
req_id 3 prompt_len 16 output_len 8 request_latency 0.28 s, first_token_latency 0.08 s
ready to request generate API http://localhost:8000/generate_stream model_dir huggyllama/llama-13b adapter_dir None len_prompt 96
ready to request generate API http://localhost:8000/generate_stream model_dir huggyllama/llama-13b adapter_dir None len_prompt 96
ready to request generate API http://localhost:8000/generate_stream model_dir huggyllama/llama-13b adapter_dir None len_prompt 96
req_id 4 prompt_len 16 output_len 8 request_latency 0.28 s, first_token_latency 0.08 s
ready to request generate API http://localhost:8000/generate_stream model_dir huggyllama/llama-13b adapter_dir None len_prompt 96
req_id 5 prompt_len 16 output_len 8 request_latency 0.49 s, first_token_latency 0.29 s
req_id 6 prompt_len 16 output_len 8 request_latency 0.39 s, first_token_latency 0.19 s
req_id 7 prompt_len 16 output_len 8 request_latency 0.29 s, first_token_latency 0.08 s
req_id 0 prompt_len 16 output_len 2040 request_latency 59.29 s, first_token_latency 0.06 s
Total time: 59.75 s
Aborted Request: 0
Throughput: 0.13 requests/s
Throughput strip: -0.81 requests/s
Average latency: 7.78 s
Average latency per token: 0.01 s
Average latency per output token: 0.05 s
Average first token latency: 0.20 s
90 percentile first token latency: < 0.41 s
50 percentile first token latency: < 0.14 s
Average satisfaction: 1.00
90 percentile satisfaction: > 1.00
50 percentile satisfaction: > 1.00
Average attainment: 1.00
